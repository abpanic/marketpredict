{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a18325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) Imports, engine check, folders\n",
    "# =========================\n",
    "\n",
    "# Core data stack + utilities; warnings silenced for cleaner notebook logs\n",
    "import pandas as pd, numpy as np, warnings, re\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Require a Parquet engine ---\n",
    "# We prefer pyarrow; if unavailable, fall back to fastparquet. If neither is present,\n",
    "# raise a clear, actionable error with pip install instructions.\n",
    "try:\n",
    "    import pyarrow  \n",
    "    PARQUET_ENGINE = \"pyarrow\"\n",
    "except ImportError:\n",
    "    try:\n",
    "        import fastparquet  \n",
    "        PARQUET_ENGINE = \"fastparquet\"\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"Parquet output is required but no engine is installed.\\n\"\n",
    "            \"Install one of:\\n\"\n",
    "            \"  pip install pyarrow\\n\"\n",
    "            \"    or\\n\"\n",
    "            \"  pip install fastparquet\"\n",
    "        ) from e\n",
    "\n",
    "# Project I/O roots:\n",
    "# - RAW: expected location for source files (CSV)\n",
    "# - PROC: destination for cleaned/engineered outputs and intermediates\n",
    "RAW  = Path('./raw')            \n",
    "PROC = Path('./processed'); PROC.mkdir(exist_ok=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94ef49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust column picker \n",
    "def pick(df, candidates):\n",
    "    \"\"\"\n",
    "    Return the first matching column name in `df` for any of the given `candidates`.\n",
    "    Tries exact match first; if none, falls back to substring (case-insensitive).\n",
    "    Example: pick(df, [\"Date\", \"date\", \"dt\"])\n",
    "    \"\"\"\n",
    "    # First pass: exact match on provided candidate names\n",
    "    for c in candidates:\n",
    "        if c in df.columns: \n",
    "            return c\n",
    "    # Second pass: substring match (case-insensitive) against all columns\n",
    "    for c in df.columns:\n",
    "        for k in candidates:\n",
    "            if k.lower() in c.lower(): \n",
    "                return c\n",
    "    # Nothing matched\n",
    "    return None\n",
    "\n",
    "\n",
    "# Smart date parser: chooses the best of multiple formats by coverage\n",
    "from dateutil import parser as _dateparser  # (Imported for completeness; not used directly)\n",
    "\n",
    "def smart_parse_dates(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Try several parsing strategies and choose the one that:\n",
    "      1) Parses the most non-null timestamps,\n",
    "      2) Yields a higher median 'months per year' coverage,\n",
    "      3) Yields a larger set of unique months overall (tie-breaker).\n",
    "    Prints a small summary of the chosen strategy.\n",
    "    \"\"\"\n",
    "    # Ensure string input for robust parsing\n",
    "    s = s.astype(str)\n",
    "\n",
    "    # Candidate parsing attempts: 'infer', 'dayfirst', and a set of explicit formats\n",
    "    attempts = []\n",
    "    attempts.append(('infer', pd.to_datetime(s, errors='coerce')))\n",
    "    attempts.append(('dayfirst', pd.to_datetime(s, errors='coerce', dayfirst=True)))\n",
    "    fmts = [\n",
    "        '%d-%b-%Y','%d-%b-%y','%d/%m/%Y','%m/%d/%Y','%Y-%m-%d',\n",
    "        '%b %d, %Y','%d %b %Y','%b %Y','%m-%Y'\n",
    "    ]\n",
    "    for fmt in fmts:\n",
    "        attempts.append((fmt, pd.to_datetime(s, format=fmt, errors='coerce')))\n",
    "\n",
    "    # Scoring function to compare parsing outcomes across attempts\n",
    "    def score(dt):\n",
    "        ok = dt.dropna()\n",
    "        if ok.empty: \n",
    "            return (0, 0, 0)\n",
    "        # Build a simple \"presence\" series indexed by parsed datetimes\n",
    "        ser = pd.Series(1, index=ok).sort_index()\n",
    "        # Count unique months present per year, then take the median across years\n",
    "        by_year_months = ser.groupby(ser.index.year).apply(\n",
    "            lambda x: len(pd.Index(x.index.month).unique())\n",
    "        )\n",
    "        med_months = int(by_year_months.median()) if len(by_year_months) else 0\n",
    "        # Return tuple: (#parsed rows, median months per year, total unique months)\n",
    "        return (len(ok), med_months, len(pd.Index(ok.dt.month).unique()))\n",
    "\n",
    "    # Pick the attempt with the best score (lexicographic max on the tuple above)\n",
    "    best_name, best_dt, best_score = max(\n",
    "        [(n, d, score(d)) for n, d in attempts], key=lambda t: t[2]\n",
    "    )\n",
    "    print(\n",
    "        f\"[smart_parse_dates] picked: {best_name} | parsed={best_score[0]} \"\n",
    "        f\"| median months/yr={best_score[1]} | unique months={best_score[2]}\"\n",
    "    )\n",
    "    return best_dt\n",
    "\n",
    "\n",
    "def parse_dates_in_df(df, date_col='Date'):\n",
    "    \"\"\"\n",
    "    Parse the `date_col` in `df` using `smart_parse_dates`, drop rows with null dates,\n",
    "    and return the DataFrame sorted by the parsed date.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    # Best-effort date parsing across multiple formats\n",
    "    out[date_col] = smart_parse_dates(out[date_col])\n",
    "    # Drop rows where date failed to parse; keep only valid dates\n",
    "    out = out.dropna(subset=[date_col]).sort_values(date_col)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _parse_investing_prices(df, date_col='Date', price_col='Price'):\n",
    "    \"\"\"\n",
    "    Clean Investing.com style price tables:\n",
    "      - Parse dates robustly,\n",
    "      - Strip thousands separators from price strings,\n",
    "      - Coerce numeric and drop non-numeric rows.\n",
    "    Returns a tidy two-column frame [date_col, price_col].\n",
    "    \"\"\"\n",
    "    out = parse_dates_in_df(df, date_col=date_col)\n",
    "\n",
    "    # Remove commas in price strings (e.g., \"12,345.67\") before numeric coercion\n",
    "    out[price_col] = pd.to_numeric(\n",
    "        out[price_col].astype(str).str.replace(',', '', regex=False), errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Drop rows where price is missing or unparsable\n",
    "    out = out.dropna(subset=[price_col])\n",
    "    return out[[date_col, price_col]]\n",
    "\n",
    "\n",
    "def investing_to_quarter_ret(df, date_col='Date', price_col='Price'):\n",
    "    \"\"\"\n",
    "    Convert (typically daily) price data into quarter-over-quarter simple returns.\n",
    "    Steps:\n",
    "      1) Clean with `_parse_investing_prices`,\n",
    "      2) Resample to quarter-end and take last price,\n",
    "      3) Compute pct_change vs. previous quarter.\n",
    "    Returns a pandas Series indexed by quarter-end.\n",
    "    \"\"\"\n",
    "    df2 = _parse_investing_prices(df, date_col, price_col)\n",
    "\n",
    "    # Use quarter-end snapshots (last trading day per quarter)\n",
    "    q_end_price = df2.set_index(date_col)[price_col].resample('Q').last()\n",
    "\n",
    "    # Quarter-over-quarter simple return\n",
    "    return q_end_price.pct_change()\n",
    "\n",
    "\n",
    "def monthly_to_quarter(series, how='mean'):\n",
    "    \"\"\"\n",
    "    Aggregate a monthly series to quarterly frequency.\n",
    "    - how='mean' (default): average within the quarter,\n",
    "    - how!='mean'      : take the last month in the quarter (quarter-end snapshot).\n",
    "    \"\"\"\n",
    "    # Choose aggregation method based on `how`\n",
    "    return series.resample('Q').mean() if how=='mean' else series.resample('Q').last()\n",
    "\n",
    "\n",
    "def yoy_from_monthly(series):\n",
    "    \"\"\"\n",
    "    Compute year-over-year % change for a monthly series.\n",
    "    Assumes the index is a DatetimeIndex at monthly frequency.\n",
    "    Returns percent values (×100).\n",
    "    \"\"\"\n",
    "    return series.pct_change(12) * 100.0\n",
    "\n",
    "\n",
    "def check_monthly_coverage(df, date_col='Date', label='series'):\n",
    "    \"\"\"\n",
    "    Print which months appear in each year and warn about completely missing quarters.\n",
    "    Helpful before resampling to ensure even coverage.\n",
    "    \"\"\"\n",
    "    # Parse dates and drop nulls for coverage accounting\n",
    "    dates = smart_parse_dates(df[date_col]).dropna()\n",
    "\n",
    "    # Presence series: one row per observed date\n",
    "    ser = pd.Series(1, index=dates).sort_index()\n",
    "\n",
    "    # List months present per year (e.g., [1, 2, 3, ...])\n",
    "    months_per_year = ser.groupby(ser.index.year).apply(\n",
    "        lambda s: sorted(pd.Index(s.index.month).unique())\n",
    "    )\n",
    "\n",
    "    print(f\"[{label}] Months present per year:\")\n",
    "    for y, months in months_per_year.items():\n",
    "        print(f\"  {y}: {months}\")\n",
    "\n",
    "    # Count number of rows per quarter; warn if any quarter has zero rows\n",
    "    q_counts = ser.resample('Q').size()\n",
    "    miss = q_counts[q_counts == 0]\n",
    "    if len(miss) > 0:\n",
    "        print(f\"[{label}] ⚠ Missing {len(miss)} quarter(s) with zero rows — resample will yield NaNs.\")\n",
    "    else:\n",
    "        print(f\"[{label}] ✅ At least one row in every quarter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "108ee50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[smart_parse_dates] picked: infer | parsed=188 | median months/yr=12 | unique months=12\n",
      "[NIFTY 50] Months present per year:\n",
      "  2010: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2011: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2012: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2013: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2014: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2015: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2016: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2017: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2018: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2019: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2020: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2021: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2022: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2023: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2024: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2025: [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[NIFTY 50] ✅ At least one row in every quarter.\n",
      "[smart_parse_dates] picked: infer | parsed=188 | median months/yr=12 | unique months=12\n",
      "[NIFTY Midcap 100] Months present per year:\n",
      "  2010: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2011: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2012: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2013: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2014: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2015: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2016: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2017: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2018: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2019: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2020: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2021: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2022: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2023: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2024: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2025: [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[NIFTY Midcap 100] ✅ At least one row in every quarter.\n",
      "[smart_parse_dates] picked: infer | parsed=188 | median months/yr=12 | unique months=12\n",
      "[smart_parse_dates] picked: infer | parsed=188 | median months/yr=12 | unique months=12\n",
      "Index quarterly points (excess_ret): 62\n"
     ]
    }
   ],
   "source": [
    "# Ensure files exist in ./raw\n",
    "# Load raw daily price data for NIFTY 50 and NIFTY Midcap 100.\n",
    "# Expected columns: 'Date' and 'Price' (string numbers ok; cleaned in helper).\n",
    "n50_raw = pd.read_csv(RAW/'Nifty50.csv')\n",
    "mid_raw = pd.read_csv(RAW/'NIFTYMidcap100.csv')\n",
    "\n",
    "# Quick coverage QA (helps catch wrong date parsing or partial exports)\n",
    "# Prints months present per year and warns if any quarter has zero rows.\n",
    "check_monthly_coverage(n50_raw, 'Date', 'NIFTY 50')\n",
    "check_monthly_coverage(mid_raw, 'Date', 'NIFTY Midcap 100')\n",
    "\n",
    "# Convert daily prices -> quarter-end prices -> quarter-over-quarter returns.\n",
    "# Uses last trading day in each quarter; simple returns via pct_change().\n",
    "nifty_qret  = investing_to_quarter_ret(n50_raw).rename('nifty_qret')\n",
    "midcap_qret = investing_to_quarter_ret(mid_raw).rename('midcap_qret')\n",
    "\n",
    "# Compute quarterly excess return (Midcap minus NIFTY).\n",
    "# Pandas aligns on the DatetimeIndex; any non-overlapping quarters yield NaN.\n",
    "excess_ret  = (midcap_qret - nifty_qret).rename('excess_ret')\n",
    "\n",
    "# Small diagnostic — how many quarters where both returns exist (non-NaN)?\n",
    "print(\"Index quarterly points (excess_ret):\", excess_ret.dropna().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71cf198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[smart_parse_dates] picked: infer | parsed=150 | median months/yr=12 | unique months=12\n",
      "CPI monthly points: 150 | CPI YoY monthly (non-NaN): 138\n",
      "CPI quarterly points: 46\n"
     ]
    }
   ],
   "source": [
    "# CPI monthly to YoY% to quarterly mean\n",
    "\n",
    "# Read CPI data (monthly frequency expected)\n",
    "cpi = pd.read_csv(RAW/'CPI_Monthly_Jan_2013_to_Jun_2025.csv')  # <- file name you uploaded\n",
    "\n",
    "# Try to locate the date and value columns even if headers vary\n",
    "c_date = pick(cpi, ['DATE','Date','Month','Period'])           # likely 'DATE'\n",
    "c_val  = pick(cpi, ['CPI','Index','Value','CPI_COMBINED_BASE2012_100'])  # likely 'CPI_COMBINED_BASE2012_100'\n",
    "\n",
    "# Defensive checks in case the expected headers aren't present\n",
    "if c_date is None or c_val is None:\n",
    "    raise KeyError(\"Could not identify CPI date/value columns; please inspect the CSV headers.\")\n",
    "\n",
    "# Parse dates robustly and sort; drop rows with unparseable dates\n",
    "# (smart_parse_dates prints the chosen strategy and coverage info)\n",
    "cpi = parse_dates_in_df(cpi, c_date)   # e.g., smart parser may pick 'infer'\n",
    "\n",
    "# Build a clean monthly CPI index series (float), indexed by datetime\n",
    "cpi_m = (\n",
    "    cpi.set_index(c_date)[c_val]\n",
    "       .astype(float)\n",
    "       .rename('cpi_index')\n",
    ")\n",
    "\n",
    "# Compute YoY % change: requires 12 months of history\n",
    "# The first 12 entries will be NaN by definition—this is expected.\n",
    "cpi_yoy_m = yoy_from_monthly(cpi_m).rename('cpi_yoy_m')\n",
    "\n",
    "# Aggregate monthly YoY to quarterly using the mean of the three months in each quarter.\n",
    "# This reduces month-to-month noise and aligns with other quarterly features/targets.\n",
    "cpi_yoy_q = monthly_to_quarter(cpi_yoy_m, how='mean').rename('cpi_yoy')\n",
    "\n",
    "# Quick diagnostics\n",
    "print(\"CPI monthly points:\", cpi_m.shape[0], \n",
    "      \"| CPI YoY monthly (non-NaN):\", cpi_yoy_m.dropna().shape[0])\n",
    "print(\"CPI quarterly points:\", cpi_yoy_q.dropna().shape[0])\n",
    "\n",
    "# persist intermediates for reuse\n",
    "# cpi_m.to_frame().to_parquet(PROC/'cpi_monthly.parquet', engine=PARQUET_ENGINE)\n",
    "# cpi_yoy_q.dropna().to_frame().to_parquet(PROC/'cpi_yoy_quarterly.parquet', engine=PARQUET_ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8fbf1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique 'quarter' samples: ['2012.0-Q1' '2012.0-Q2' '2012.0-Q3' '2012.0-Q4' '2013.0-Q1' '2013.0-Q2'\n",
      " '2013.0-Q3' '2013.0-Q4']\n",
      "Normalized samples: ['2012-Q1' '2012-Q2' '2012-Q3' '2012-Q4' '2013-Q1' '2013-Q2' '2013-Q3'\n",
      " '2013-Q4']\n",
      "GDP quarterly points: 50\n",
      "GDP range: 2013-03-31 → 2025-06-30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "__q\n",
       "2013-03-31    4.296010\n",
       "2013-06-30    6.447099\n",
       "2013-09-30    7.337749\n",
       "2013-12-31    6.534983\n",
       "Freq: QE-DEC, Name: gdp_yoy, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- GDP quarterly loader (fixes '2012.0-Q1' etc.) ---\n",
    "\n",
    "import re\n",
    "from pandas.tseries.offsets import QuarterEnd  # imported if you later need explicit quarter-end arithmetic\n",
    "\n",
    "# Read the raw GDP table\n",
    "gdp_raw = pd.read_csv(RAW/'GDP_Quarterly_2010_2025.csv').copy()\n",
    "\n",
    "# 1) Clean numeric column\n",
    "# Expect a GDP \"level\" column named 'gdp'; coerce strings like \"1,234.5\" into float.\n",
    "if 'gdp' not in gdp_raw.columns:\n",
    "    raise ValueError(\"Expected a 'gdp' column in GDP_Quarterly_2010_2025.csv\")\n",
    "\n",
    "gdp_raw['gdp_clean'] = pd.to_numeric(\n",
    "    gdp_raw['gdp'].astype(str).str.replace(',', '', regex=False),\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# 2) Normalize quarter strings (strip the '.0' before '-Q')\n",
    "# Expect some variant of a quarter label in a column named 'quarter'.\n",
    "if 'quarter' not in gdp_raw.columns:\n",
    "    raise ValueError(\"Expected a 'quarter' column in GDP_Quarterly_2010_2025.csv\")\n",
    "\n",
    "# Remove trailing \".0\" artifacts that sometimes appear after numeric-to-string conversions.\n",
    "gdp_raw['quarter_str'] = gdp_raw['quarter'].astype(str).str.replace('.0', '', regex=False)\n",
    "\n",
    "# 3) Extract YYYY and Qn from anything like '2012-Q1', '2012Q1', 'Q1 2012.0', etc.\n",
    "# Use a permissive regex to find 4-digit year followed by a 'Q[1-4]' anywhere in the string.\n",
    "m = gdp_raw['quarter_str'].str.extract(r'(?P<y>\\d{4}).*?Q(?P<q>[1-4])')\n",
    "\n",
    "# Build canonical tokens like \"2012Q1\" and mark valid rows\n",
    "qstr = (m['y'].fillna('') + 'Q' + m['q'].fillna(''))\n",
    "valid = qstr.str.match(r'^\\d{4}Q[1-4]$')\n",
    "\n",
    "# Keep only rows that parse into a clean Year+Quarter token\n",
    "gdp_norm = gdp_raw.loc[valid].copy()\n",
    "\n",
    "# Convert to quarter-end timestamps using PeriodIndex (calendar quarters)\n",
    "gdp_norm['__q'] = pd.PeriodIndex(qstr[valid], freq='Q').to_timestamp(how='end')\n",
    "\n",
    "# 4) Build a clean quarterly level series (last obs per quarter if duplicates)\n",
    "# If multiple rows map to the same quarter, take the last one (common when data is revised).\n",
    "gdp_q_level = (gdp_norm\n",
    "               .dropna(subset=['__q','gdp_clean'])\n",
    "               .set_index('__q')\n",
    "               .sort_index()\n",
    "               .groupby(pd.Grouper(freq='Q'))['gdp_clean']\n",
    "               .last())\n",
    "\n",
    "# 5) Compute YoY from levels (4-quarter change * 100)\n",
    "# This assumes levels are comparable across quarters (same base/deflator).\n",
    "gdp_q = gdp_q_level.pct_change(4).mul(100.0).rename('gdp_yoy').dropna()\n",
    "\n",
    "# 6) Diagnostics\n",
    "print(\"Unique 'quarter' samples:\", gdp_raw['quarter'].astype(str).unique()[:8])\n",
    "print(\"Normalized samples:\", gdp_norm['quarter_str'].astype(str).unique()[:8])\n",
    "print(\"GDP quarterly points:\", gdp_q.shape[0])\n",
    "if not gdp_q.empty:\n",
    "    print(\"GDP range:\", gdp_q.index.min().date(), \"→\", gdp_q.index.max().date())\n",
    "display(gdp_q.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc385914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[smart_parse_dates] picked: infer | parsed=188 | median months/yr=12 | unique months=12\n",
      "[Repo] months covered: 12 median months/yr; quarters: 63 (2010-03-31 → 2025-09-30)\n",
      "2024-09-30    6.50\n",
      "2024-12-31    6.50\n",
      "2025-03-31    6.25\n",
      "2025-06-30    5.50\n",
      "2025-09-30    5.50\n",
      "Freq: QE-DEC, Name: repo, dtype: float64\n",
      "2024-09-30     0.0\n",
      "2024-12-31     0.0\n",
      "2025-03-31   -25.0\n",
      "2025-06-30   -75.0\n",
      "2025-09-30     0.0\n",
      "Freq: QE-DEC, Name: repo_chg_bps, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def load_repo_monthly_to_quarter(path: Path):\n",
    "    raw = pd.read_csv(path)\n",
    "\n",
    "    # 1) find date & rate columns\n",
    "    dcol = pick(raw, ['DATE','Date','Month','Period'])\n",
    "    vcol = pick(raw, ['REPO_RATE_PERCENT','Repo','Rate','Policy Rate','REPO'])\n",
    "    if dcol is None or vcol is None:\n",
    "        raise ValueError(\"Repo CSV must have a date column (DATE/Month/Period) and a value column (Repo/Rate/REPO_RATE_PERCENT).\")\n",
    "\n",
    "    # 2) parse dates & clean numeric\n",
    "    df = parse_dates_in_df(raw, dcol)\n",
    "\n",
    "    # Remove thousands separators and '%' symbols; strip whitespace (use .str.strip(), not .str.trim())\n",
    "    # Also nuke zero-width spaces that sometimes sneak in from copy-paste exports.\n",
    "    val = (df[vcol].astype(str)\n",
    "                  .str.replace('\\u200b', '', regex=False)\n",
    "                  .str.replace(',', '', regex=False)\n",
    "                  .str.replace('%', '', regex=False)\n",
    "                  .str.strip())\n",
    "    val = pd.to_numeric(val, errors='coerce')\n",
    "\n",
    "    # 3) detect scaling (percent vs fraction)\n",
    "    # Typical RBI repo is ~4–10; if values look like 0.04–0.10, scale by 100 to percent.\n",
    "    median_abs = val.dropna().abs().median()\n",
    "    if pd.notna(median_abs) and median_abs < 1:\n",
    "        val = val * 100.0\n",
    "\n",
    "    # Attach cleaned numeric, dropping rows that still fail numeric coercion\n",
    "    df = df.assign(val=val).dropna(subset=['val'])\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Repo series is empty after cleaning; check the source file/headers.\")\n",
    "\n",
    "    # 4) normalize to month-end stamps, forward-fill missing months\n",
    "    m_end = (df.set_index(dcol)['val']\n",
    "               .to_period('M').to_timestamp('M')\n",
    "               .sort_index())\n",
    "\n",
    "    # Create a complete month-end index and forward-fill inside the series\n",
    "    full_m = pd.date_range(m_end.index.min(), m_end.index.max(), freq='M')\n",
    "    m_end = m_end.reindex(full_m).ffill()\n",
    "\n",
    "    # 5) quarter-end level & QoQ change in bps\n",
    "    repo_q = m_end.resample('Q').last().rename('repo')                 # % at quarter-end\n",
    "    repo_chg_bps = (repo_q.diff() * 100.0).rename('repo_chg_bps')      # Δ in basis points\n",
    "\n",
    "    # 6) coverage diagnostics\n",
    "    by_year_months = pd.Series(1, index=full_m).groupby(pd.Grouper(freq='Y')).size()\n",
    "    print(f\"[Repo] months covered: {int(by_year_months.median())} median months/yr; \"\n",
    "          f\"quarters: {repo_q.shape[0]} ({repo_q.index.min().date()} → {repo_q.index.max().date()})\")\n",
    "\n",
    "    return repo_q, repo_chg_bps\n",
    "\n",
    "# ---- run the loader\n",
    "repo_q, repo_chg_bps = load_repo_monthly_to_quarter(RAW/'Repo_Rate_Monthly_2010_2025.csv')\n",
    "\n",
    "# quick peek\n",
    "print(repo_q.tail())\n",
    "print(repo_chg_bps.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99452c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rainfall seasonal years covered: [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025]\n",
      "Rainfall quarterly points: 53 | Range: 2012-09-30 → 2025-09-30\n"
     ]
    }
   ],
   "source": [
    "# ---- Rainfall seasonal anomaly from your monthly file (2012–2025) ----\n",
    "# File columns (confirmed): year, month (\"Jan\"..\"Dec\"), rainfall_mm, good_rainfall_mm, anomaly_mm\n",
    "\n",
    "rain = pd.read_csv(RAW/'AnnualRainfall_with_Good_and_Anomaly_2012_2025.csv')\n",
    "\n",
    "# Flexible column picks (if you already have pick(), you can use it; here we reference directly)\n",
    "ycol, mcol = 'year', 'month'\n",
    "obs_col, norm_col, mm_anom_col = 'rainfall_mm', 'good_rainfall_mm', 'anomaly_mm'  # we will *not* use anomaly_mm\n",
    "\n",
    "# Normalize month text -> month number (1..12)\n",
    "mmap = {m[:3].lower(): i for i, m in enumerate(\n",
    "    ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'], start=1)}\n",
    "rain['mon'] = rain[mcol].astype(str).str[:3].str.lower().map(mmap).astype(int)\n",
    "\n",
    "# Keep Southwest monsoon months: Jun(6)–Sep(9)\n",
    "mons = rain[rain['mon'].between(6, 9)].copy()\n",
    "\n",
    "# --- QA 1: ensure each monsoon season has its 4 months ---\n",
    "# If a year has missing months (e.g., data gaps), flag it early.\n",
    "miss = (mons.groupby(ycol)['mon']\n",
    "            .nunique()\n",
    "            .rename('monsoon_months')\n",
    "            .reset_index())\n",
    "bad = miss[miss['monsoon_months'] < 4]\n",
    "if not bad.empty:\n",
    "    print(\"⚠ Monsoon months missing in years:\", bad[ycol].tolist())\n",
    "\n",
    "# Aggregate to seasonal totals per year (use *observed* and *normal*; ignore anomaly_mm to avoid sign confusion)\n",
    "grp = mons.groupby(ycol, as_index=False).agg(\n",
    "    obs_mm  = (obs_col,  'sum'),\n",
    "    norm_mm = (norm_col, 'sum'),\n",
    "    # anom_mm_sum = (mm_anom_col, 'sum')  # optional: cross-check below\n",
    ")\n",
    "\n",
    "# IMD-style anomaly%: (observed - normal)/normal * 100  (positive = above normal)\n",
    "# NOTE: norm_mm should be > 0; if zeros exist, handle before division to avoid inf/-inf.\n",
    "grp['rain_anom_pct'] = (grp['obs_mm'] - grp['norm_mm']) / grp['norm_mm'] * 100.0\n",
    "\n",
    "# cross-check using your provided anomaly_mm (good - observed)\n",
    "# If you want to verify signs, uncomment:\n",
    "# check = mons.groupby(ycol, as_index=False).agg(anom_mm_sum=(mm_anom_col, 'sum'),\n",
    "#                                                norm_mm_sum=(norm_col, 'sum'))\n",
    "# check['alt_pct_from_file'] = (-check['anom_mm_sum'] / check['norm_mm_sum']) * 100.0\n",
    "# print(check[[ycol,'alt_pct_from_file']].head())\n",
    "\n",
    "# Stamp each year's anomaly at Sep-30; forward-fill within the same year → quarterly series\n",
    "# This holds the SW monsoon signal from Q3 (Sep) through Q4/Q1/Q2 until the next Sep stamp.\n",
    "rain_idx = pd.to_datetime(grp[ycol].astype(int).astype(str) + '-09-30')\n",
    "rain_q = (pd.Series(grp['rain_anom_pct'].values, index=rain_idx)\n",
    "            .resample('Q').ffill()\n",
    "            .rename('rain_anom'))\n",
    "\n",
    "print(\"Rainfall seasonal years covered:\", grp[ycol].tolist())\n",
    "print(\"Rainfall quarterly points:\", rain_q.shape[0], \"| Range:\", rain_q.index.min().date(), \"→\", rain_q.index.max().date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b155a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final quarterly rows: 44 | Range: 2014-06-30 → 2025-03-31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>midcap_qret</th>\n",
       "      <th>nifty_qret</th>\n",
       "      <th>excess_ret</th>\n",
       "      <th>rain_anom</th>\n",
       "      <th>cpi_yoy</th>\n",
       "      <th>gdp_yoy</th>\n",
       "      <th>repo_chg_bps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-06-30</th>\n",
       "      <td>0.288472</td>\n",
       "      <td>0.135311</td>\n",
       "      <td>0.153161</td>\n",
       "      <td>69.879102</td>\n",
       "      <td>7.859486</td>\n",
       "      <td>8.023963</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-30</th>\n",
       "      <td>0.028963</td>\n",
       "      <td>0.046437</td>\n",
       "      <td>-0.017474</td>\n",
       "      <td>-7.449626</td>\n",
       "      <td>6.681568</td>\n",
       "      <td>8.704109</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31</th>\n",
       "      <td>0.102077</td>\n",
       "      <td>0.039913</td>\n",
       "      <td>0.062164</td>\n",
       "      <td>-7.449626</td>\n",
       "      <td>4.054538</td>\n",
       "      <td>5.922736</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>0.033169</td>\n",
       "      <td>0.025149</td>\n",
       "      <td>0.008021</td>\n",
       "      <td>-7.449626</td>\n",
       "      <td>5.272440</td>\n",
       "      <td>7.112080</td>\n",
       "      <td>-50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>0.000646</td>\n",
       "      <td>-0.014427</td>\n",
       "      <td>0.015073</td>\n",
       "      <td>-7.449626</td>\n",
       "      <td>5.090809</td>\n",
       "      <td>7.592544</td>\n",
       "      <td>-25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>-0.001933</td>\n",
       "      <td>-0.050140</td>\n",
       "      <td>0.048207</td>\n",
       "      <td>-6.171560</td>\n",
       "      <td>3.948304</td>\n",
       "      <td>8.033806</td>\n",
       "      <td>-50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            midcap_qret  nifty_qret  excess_ret  rain_anom   cpi_yoy  \\\n",
       "2014-06-30     0.288472    0.135311    0.153161  69.879102  7.859486   \n",
       "2014-09-30     0.028963    0.046437   -0.017474  -7.449626  6.681568   \n",
       "2014-12-31     0.102077    0.039913    0.062164  -7.449626  4.054538   \n",
       "2015-03-31     0.033169    0.025149    0.008021  -7.449626  5.272440   \n",
       "2015-06-30     0.000646   -0.014427    0.015073  -7.449626  5.090809   \n",
       "2015-09-30    -0.001933   -0.050140    0.048207  -6.171560  3.948304   \n",
       "\n",
       "             gdp_yoy  repo_chg_bps  \n",
       "2014-06-30  8.023963           0.0  \n",
       "2014-09-30  8.704109           0.0  \n",
       "2014-12-31  5.922736           0.0  \n",
       "2015-03-31  7.112080         -50.0  \n",
       "2015-06-30  7.592544         -25.0  \n",
       "2015-09-30  8.033806         -50.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Build the modeling DataFrame on the shared quarterly intersection\n",
    "# Use an INNER join so we only keep quarters present in *all* component series.\n",
    "parts = [excess_ret, midcap_qret, nifty_qret, rain_q, cpi_yoy_q, gdp_q, repo_chg_bps]\n",
    "qdf = pd.concat(parts, axis=1, join='inner')  # inner = strict intersection of all indices\n",
    "\n",
    "# Normalize index to *quarter-end* timestamps (calendar quarters) for consistency\n",
    "qdf.index = qdf.index.to_period('Q').to_timestamp('Q')\n",
    "\n",
    "# 2) Create lags (t-1) and the prediction target (t+1)\n",
    "# Feature lags: use previous quarter's information to predict the next quarter\n",
    "qdf['ret_prev_q']    = qdf['midcap_qret'].shift(1)   # prior quarter midcap return\n",
    "qdf['rain_anom_lag'] = qdf['rain_anom'].shift(1)     # prior monsoon anomaly %\n",
    "qdf['cpi_yoy_lag']   = qdf['cpi_yoy'].shift(1)       # prior quarter CPI YoY %\n",
    "qdf['gdp_yoy_lag']   = qdf['gdp_yoy'].shift(1)       # prior quarter GDP YoY %\n",
    "qdf['repo_chg_lag']  = qdf['repo_chg_bps'].shift(1)  # prior quarter repo Δ (bps)\n",
    "\n",
    "# Target: next quarter's midcap-vs-nifty *excess* return (t+1)\n",
    "qdf['excess_next_q'] = qdf['excess_ret'].shift(-1)\n",
    "\n",
    "# 3) Clean rows created by shifting and print a quick QA\n",
    "# Drop any rows with NaNs introduced by the shifts (first/last quarters)\n",
    "qdf = qdf.dropna(how=\"any\").copy()\n",
    "\n",
    "# Basic coverage diagnostic\n",
    "print(\"Final quarterly rows:\", len(qdf), \n",
    "      \"| Range:\", qdf.index.min().date(), \"→\", qdf.index.max().date())\n",
    "\n",
    "# Peek at the main contemporaneous series to confirm values look reasonable\n",
    "qdf[['midcap_qret','nifty_qret','excess_ret','rain_anom','cpi_yoy','gdp_yoy','repo_chg_bps']].head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25ea9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdf_out = qdf.copy()\n",
    "\n",
    "# Re-derive target: next-quarter excess return (t+1).\n",
    "# NOTE: This duplicates the earlier definition; harmless but redundant.\n",
    "#qdf_out['excess_next_q'] = qdf_out['excess_ret'].shift(-1)\n",
    "\n",
    "# Define \"previous quarter\" return feature.\n",
    "# ⚠ NOTE: shift(-1) uses the *next* quarter's value, which introduces look-ahead leakage.\n",
    "# If you truly intend t-1, use shift(1). See \"After\" notes below.\n",
    "qdf_out['ret_prev_q']    = qdf_out['midcap_qret'].shift(1)  \n",
    "\n",
    "# Build a version for export with a materialized date column (quarter-end timestamp)\n",
    "# The reset_index() will turn the DatetimeIndex into a column named 'index' (since the index has no name).\n",
    "# We then rename it to 'date_q' for clarity.\n",
    "qdf_final = qdf_out.reset_index().rename(columns={'index':'date_q'}).to_csv(PROC/'quarterly_features.csv', index=False)\n",
    "\n",
    "\n",
    "# Write out a CSV artifact for downstream modeling.\n",
    "qdf_out.to_parquet(PROC/'quarterly_features.parquet', engine=PARQUET_ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7192e6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[smart_parse_dates] picked: infer | parsed=188 | med months/yr=12 | unique months=12\n",
      "[smart_parse_dates] picked: infer | parsed=188 | med months/yr=12 | unique months=12\n",
      "[IDX] Months: 187 | Range: 2010-01-31 → 2025-08-31\n",
      "[smart_parse_dates] picked: infer | parsed=150 | med months/yr=12 | unique months=12\n",
      "[CPI] Monthly points: 150 | YoY non-NaN: 138\n",
      "[GDP] Q points: 50 | expanded monthly points: 150 | sample: 2013-01-31 → 2025-06-30\n",
      "[smart_parse_dates] picked: infer | parsed=188 | med months/yr=12 | unique months=12\n",
      "[REPO] Months: 188 | Range: 2010-01-31 → 2025-08-31\n",
      "[RAIN] picks → Year=year, Month=month, Obs=rainfall_mm, Norm=good_rainfall_mm\n",
      "[RAIN] NaN check → total: 168 | non-NaN: 0 | unique years: 14 | range: 2012-01-31 → 2025-12-31\n",
      "\n",
      "--- Coverage before join ---\n",
      "[excess_ret  ] non-NaN= 187 | 2010-02-28 → 2025-08-31\n",
      "[midcap_mret ] non-NaN= 187 | 2010-02-28 → 2025-08-31\n",
      "[nifty_mret  ] non-NaN= 187 | 2010-02-28 → 2025-08-31\n",
      "[cpi_yoy     ] non-NaN= 138 | 2014-01-31 → 2025-06-30\n",
      "[gdp_yoy     ] non-NaN= 150 | 2013-01-31 → 2025-06-30\n",
      "[repo_pct    ] non-NaN= 188 | 2010-01-31 → 2025-08-31\n",
      "[repo_chg_bps] non-NaN= 187 | 2010-02-28 → 2025-08-31\n",
      "[rain_anom_m ] non-NaN=   0 | — → —\n",
      "[rain_anom_3m] non-NaN=   0 | — → —\n",
      "----------------------------\n",
      "\n",
      "[FINAL] Monthly rows: 0 | NaT → NaT\n",
      "Saved: C:\\Users\\abpanick\\OneDrive - Microsoft\\Documents\\GitHub\\marketpredict\\processed\\monthly_features.csv and C:\\Users\\abpanick\\OneDrive - Microsoft\\Documents\\GitHub\\marketpredict\\processed\\monthly_features.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>excess_ret</th>\n",
       "      <th>midcap_mret</th>\n",
       "      <th>nifty_mret</th>\n",
       "      <th>cpi_yoy</th>\n",
       "      <th>gdp_yoy</th>\n",
       "      <th>repo_pct</th>\n",
       "      <th>repo_chg_bps</th>\n",
       "      <th>rain_anom_m</th>\n",
       "      <th>rain_anom_3m</th>\n",
       "      <th>ret_prev_m</th>\n",
       "      <th>rain_anom_lag</th>\n",
       "      <th>rain_anom_3m_lag</th>\n",
       "      <th>cpi_yoy_lag</th>\n",
       "      <th>gdp_yoy_lag</th>\n",
       "      <th>repo_chg_lag</th>\n",
       "      <th>excess_next_m</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_m</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [excess_ret, midcap_mret, nifty_mret, cpi_yoy, gdp_yoy, repo_pct, repo_chg_bps, rain_anom_m, rain_anom_3m, ret_prev_m, rain_anom_lag, rain_anom_3m_lag, cpi_yoy_lag, gdp_yoy_lag, repo_chg_lag, excess_next_m]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================\n",
    "# Monthly FeatureBuilder (M)\n",
    "# ==============================\n",
    "import pandas as pd, numpy as np, warnings, re\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RAW  = Path('./raw')\n",
    "PROC = Path('./processed'); PROC.mkdir(exist_ok=True)\n",
    "\n",
    "# ---- Parquet engine (optional) ----\n",
    "try:\n",
    "    import pyarrow  # noqa\n",
    "    PARQUET_ENGINE = \"pyarrow\"\n",
    "except Exception:\n",
    "    try:\n",
    "        import fastparquet  # noqa\n",
    "        PARQUET_ENGINE = \"fastparquet\"\n",
    "    except Exception:\n",
    "        PARQUET_ENGINE = None\n",
    "\n",
    "# ---- Safe display (optional) ----\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    def display(x): print(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers (robust & re-used)\n",
    "# -----------------------------\n",
    "def pick(df_or_cols, candidates):\n",
    "    \"\"\"Return first matching column from df.columns or list of columns.\n",
    "       Exact > longest substring.\"\"\"\n",
    "    cols = list(df_or_cols.columns) if hasattr(df_or_cols, \"columns\") else list(df_or_cols)\n",
    "    # exact match first\n",
    "    for c in candidates:\n",
    "        if c in cols: return c\n",
    "    # then longest substring match\n",
    "    for k in sorted(candidates, key=len, reverse=True):\n",
    "        for c in cols:\n",
    "            if k.lower() in c.lower(): return c\n",
    "    return None\n",
    "\n",
    "def smart_parse_dates(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str)\n",
    "    attempts = []\n",
    "    attempts.append(('infer', pd.to_datetime(s, errors='coerce')))\n",
    "    attempts.append(('dayfirst', pd.to_datetime(s, errors='coerce', dayfirst=True)))\n",
    "    fmts = ['%d-%b-%Y','%d-%b-%y','%d/%m/%Y','%m/%d/%Y','%Y-%m-%d','%b %d, %Y','%d %b %Y','%b %Y','%m-%Y']\n",
    "    for fmt in fmts:\n",
    "        attempts.append((fmt, pd.to_datetime(s, format=fmt, errors='coerce')))\n",
    "    def score(dt):\n",
    "        ok = dt.dropna()\n",
    "        if ok.empty: return (0,0,0)\n",
    "        ser = pd.Series(1, index=ok).sort_index()\n",
    "        by_year_months = ser.groupby(ser.index.year).apply(lambda x: len(pd.Index(x.index.month).unique()))\n",
    "        med_months = int(by_year_months.median()) if len(by_year_months) else 0\n",
    "        return (len(ok), med_months, len(pd.Index(ok.dt.month).unique()))\n",
    "    best_name, best_dt, best_score = max([(n,d,score(d)) for n,d in attempts], key=lambda t: t[2])\n",
    "    print(f\"[smart_parse_dates] picked: {best_name} | parsed={best_score[0]} | med months/yr={best_score[1]} | unique months={best_score[2]}\")\n",
    "    return best_dt\n",
    "\n",
    "def parse_dates_in_df(df, date_col):\n",
    "    out = df.copy()\n",
    "    out[date_col] = smart_parse_dates(out[date_col])\n",
    "    return out.dropna(subset=[date_col]).sort_values(date_col)\n",
    "\n",
    "def _parse_investing_prices(df, date_col='Date', price_col='Price'):\n",
    "    out = parse_dates_in_df(df, date_col=date_col)\n",
    "    out[price_col] = (out[price_col].astype(str)\n",
    "                                   .str.replace(r'[,\\s₹$]', '', regex=True)\n",
    "                                   .str.replace(r'[^0-9.\\-]', '', regex=True))\n",
    "    out[price_col] = pd.to_numeric(out[price_col], errors='coerce')\n",
    "    return out.dropna(subset=[price_col])[[date_col, price_col]]\n",
    "\n",
    "def prices_to_monthly_ret(df, date_col='Date', price_col='Price'):\n",
    "    \"\"\"Monthly pct change from month-end closes (last available trading day).\"\"\"\n",
    "    p = (_parse_investing_prices(df, date_col, price_col)\n",
    "           .set_index(date_col)[price_col]\n",
    "           .sort_index())\n",
    "    m_close = p.resample('M').last()           # last trading day each month\n",
    "    return m_close.pct_change().rename('mret')\n",
    "\n",
    "def yoy_from_monthly(series):\n",
    "    return series.pct_change(12) * 100.0\n",
    "\n",
    "def to_numeric_clean(s):\n",
    "    \"\"\"Coerce to float after stripping commas, spaces, NBSP and non-numeric junk.\"\"\"\n",
    "    return (pd.Series(s, copy=False).astype(str)\n",
    "              .str.replace(r'[\\u00A0\\s,]', '', regex=True)   # spaces/NBSP/commas\n",
    "              .str.replace(r'[^0-9.\\-]', '', regex=True)     # keep digits dot minus\n",
    "              .replace({'': np.nan})\n",
    "              .astype(float))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Index returns (monthly)\n",
    "# -----------------------------\n",
    "n50_raw = pd.read_csv(RAW/'Nifty50.csv')\n",
    "mid_raw = pd.read_csv(RAW/'NIFTYMidcap100.csv')\n",
    "\n",
    "nifty_mret  = prices_to_monthly_ret(n50_raw).rename('nifty_mret')\n",
    "midcap_mret = prices_to_monthly_ret(mid_raw).rename('midcap_mret')\n",
    "excess_ret  = (midcap_mret - nifty_mret).rename('excess_ret')\n",
    "\n",
    "print(\"[IDX] Months:\", excess_ret.dropna().shape[0], \"| Range:\", excess_ret.index.min().date(), \"→\", excess_ret.index.max().date())\n",
    "\n",
    "# -----------------------------\n",
    "# 2) CPI (monthly YoY)\n",
    "# -----------------------------\n",
    "cpi = pd.read_csv(RAW/'CPI_Monthly_Jan_2013_to_Jun_2025.csv')\n",
    "c_date = pick(cpi, ['DATE','Date','Month','Period'])\n",
    "c_val  = pick(cpi, ['CPI_COMBINED_BASE2012_100','CPI','Index','Value'])\n",
    "cpi = parse_dates_in_df(cpi, c_date)\n",
    "\n",
    "cpi_m = (to_numeric_clean(cpi.set_index(c_date)[c_val])\n",
    "           .to_period('M').to_timestamp('M')\n",
    "           .rename('cpi_index'))\n",
    "cpi_yoy_m = yoy_from_monthly(cpi_m).rename('cpi_yoy')\n",
    "\n",
    "print(\"[CPI] Monthly points:\", cpi_m.shape[0], \"| YoY non-NaN:\", cpi_yoy_m.dropna().shape[0])\n",
    "\n",
    "# -----------------------------\n",
    "# 3) GDP (quarterly YoY → monthly)\n",
    "# -----------------------------\n",
    "gdp = pd.read_csv(RAW/'GDP_Quarterly_2010_2025.csv').copy()\n",
    "if 'gdp' not in gdp.columns or 'quarter' not in gdp.columns:\n",
    "    raise ValueError(\"GDP CSV must contain columns: 'gdp' and 'quarter'.\")\n",
    "\n",
    "gdp['gdp_clean'] = to_numeric_clean(gdp['gdp'])\n",
    "gdp['quarter_str'] = gdp['quarter'].astype(str).str.replace('.0', '', regex=False)\n",
    "m = gdp['quarter_str'].str.extract(r'(?P<y>\\d{4}).*?Q(?P<q>[1-4])')\n",
    "qstr = (m['y'].fillna('') + 'Q' + m['q'].fillna(''))\n",
    "valid = qstr.str.match(r'^\\d{4}Q[1-4]$')\n",
    "\n",
    "gdp_norm = gdp.loc[valid & gdp['gdp_clean'].notna()].copy()\n",
    "gdp_norm['q_end'] = pd.PeriodIndex(qstr[valid], freq='Q').to_timestamp(how='end')\n",
    "\n",
    "# quarterly level → YoY%\n",
    "gdp_q_level = (gdp_norm.set_index('q_end')['gdp_clean']\n",
    "                         .sort_index()\n",
    "                         .groupby(pd.Grouper(freq='Q')).last())\n",
    "gdp_yoy_q = gdp_q_level.pct_change(4).mul(100.0).dropna().rename('gdp_yoy')\n",
    "\n",
    "# Expand each quarter’s YoY to its three month-ends explicitly\n",
    "def repeat_quarter_to_months(q_series: pd.Series) -> pd.Series:\n",
    "    rows = []\n",
    "    for q_end, val in q_series.items():\n",
    "        m3 = q_end\n",
    "        m2 = q_end - pd.offsets.MonthEnd(1)   # prior month-end\n",
    "        m1 = q_end - pd.offsets.MonthEnd(2)   # two months back\n",
    "        rows.append(pd.Series([val, val, val], index=[m1, m2, m3]))\n",
    "    return pd.concat(rows).sort_index()\n",
    "\n",
    "gdp_yoy_m = repeat_quarter_to_months(gdp_yoy_q).rename('gdp_yoy')\n",
    "gdp_yoy_m = gdp_yoy_m[~gdp_yoy_m.index.duplicated(keep='last')].sort_index()\n",
    "\n",
    "print(\"[GDP] Q points:\", gdp_yoy_q.shape[0], \n",
    "      \"| expanded monthly points:\", gdp_yoy_m.shape[0], \n",
    "      \"| sample:\", gdp_yoy_m.index.min().date(), \"→\", gdp_yoy_m.index.max().date())\n",
    "\n",
    "# (Optional realism) – publication lag by 1 month:\n",
    "# gdp_yoy_m = gdp_yoy_m.shift(1)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Repo (monthly level % + Δbps MoM)\n",
    "# -----------------------------\n",
    "repo = pd.read_csv(RAW/'Repo_Rate_Monthly_2010_2025.csv')\n",
    "dcol = pick(repo, ['DATE','Date','Month','Period'])\n",
    "vcol = pick(repo, ['REPO_RATE_PERCENT','Repo','Rate','Policy Rate','REPO'])\n",
    "if dcol is None or vcol is None:\n",
    "    raise ValueError(\"Repo CSV must have a date column (DATE/Month/Period) and a value column (Repo/Rate/REPO_RATE_PERCENT).\")\n",
    "\n",
    "repo = parse_dates_in_df(repo, dcol)\n",
    "r_val = (repo[vcol].astype(str)\n",
    "                  .str.replace(r'[,\\s%]', '', regex=True)\n",
    "                  .str.replace(r'[^0-9.\\-]', '', regex=True)\n",
    "                  .str.strip())\n",
    "r_val = pd.to_numeric(r_val, errors='coerce')\n",
    "\n",
    "# scale detect: if ~0.06 → 6.0%\n",
    "if r_val.dropna().abs().median() < 1:\n",
    "    r_val = r_val * 100.0\n",
    "\n",
    "repo_m = (pd.Series(r_val.values, index=repo[dcol])\n",
    "            .to_period('M').to_timestamp('M')\n",
    "            .sort_index()\n",
    "            .resample('M').last()\n",
    "            .ffill()\n",
    "            .rename('repo'))\n",
    "repo_m = repo_m[~repo_m.index.duplicated(keep='last')]\n",
    "repo_chg_bps_m = repo_m.diff().mul(100.0).rename('repo_chg_bps')\n",
    "\n",
    "print(\"[REPO] Months:\", repo_m.shape[0], \"| Range:\", repo_m.index.min().date(), \"→\", repo_m.index.max().date())\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Rainfall (monthly anomaly %)\n",
    "# -----------------------------\n",
    "rain = pd.read_csv(RAW/'AnnualRainfall_with_Good_and_Anomaly_2012_2025.csv')\n",
    "\n",
    "# Column picks (be explicit & log)\n",
    "ycol = pick(rain, ['Year','year'])\n",
    "mcol = pick(rain, ['month','Month'])   # may be 'month (Jan..Dec)'\n",
    "obs_col  = pick(rain, ['rainfall_mm','Observed','Rainfall'])\n",
    "norm_col = pick(rain, ['good_rainfall_mm','Normal','Climatology'])\n",
    "print(f\"[RAIN] picks → Year={ycol}, Month={mcol}, Obs={obs_col}, Norm={norm_col}\")\n",
    "\n",
    "# Month label -> month number; make month-end index\n",
    "mmap = {m[:3].lower(): i for i, m in enumerate(\n",
    "    ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'], start=1)}\n",
    "rain['mon']  = rain[mcol].astype(str).str[:3].str.lower().map(mmap).astype(int)\n",
    "rain['year'] = to_numeric_clean(rain[ycol]).astype('Int64')\n",
    "\n",
    "rain_idx = pd.to_datetime(rain['year'].astype(str) + '-' + rain['mon'].astype(str) + '-01') + pd.offsets.MonthEnd(0)\n",
    "\n",
    "# Robust numeric clean for observed/normal\n",
    "obs  = to_numeric_clean(rain[obs_col])\n",
    "norm = to_numeric_clean(rain[norm_col])\n",
    "\n",
    "# Avoid divide-by-zero\n",
    "norm = norm.replace(0.0, np.nan)\n",
    "\n",
    "rain_anom_m = pd.Series((obs - norm) / norm * 100.0, index=rain_idx, name='rain_anom_m').sort_index()\n",
    "# If duplicate month-ends exist, keep the last (or .mean() if needed)\n",
    "rain_anom_m = (rain_anom_m.groupby(rain_anom_m.index).last()).sort_index()\n",
    "# Smooth option: 3M rolling\n",
    "rain_anom_3m = rain_anom_m.rolling(3, min_periods=1).mean().rename('rain_anom_3m')\n",
    "\n",
    "# Option A — use the Series index (DatetimeIndex)\n",
    "print(\"[RAIN] NaN check → total:\", len(rain_anom_m),\n",
    "      \"| non-NaN:\", rain_anom_m.dropna().shape[0],\n",
    "      \"| unique years:\", rain_anom_m.index.year.nunique(),\n",
    "      \"| range:\", rain_anom_m.index.min().date(), \"→\", rain_anom_m.index.max().date())\n",
    "\n",
    "def report_series(name, s):\n",
    "    sd = s.dropna()\n",
    "    start = sd.index.min().date() if not sd.empty else '—'\n",
    "    end   = sd.index.max().date() if not sd.empty else '—'\n",
    "    print(f\"[{name:12}] non-NaN={len(sd):4d} | {start} → {end}\")\n",
    "\n",
    "print(\"\\n--- Coverage before join ---\")\n",
    "report_series('excess_ret',   excess_ret)\n",
    "report_series('midcap_mret',  midcap_mret)\n",
    "report_series('nifty_mret',   nifty_mret)\n",
    "report_series('cpi_yoy',      cpi_yoy_m)\n",
    "report_series('gdp_yoy',      gdp_yoy_m)\n",
    "report_series('repo_pct',     repo_m)\n",
    "report_series('repo_chg_bps', repo_chg_bps_m)\n",
    "report_series('rain_anom_m',  rain_anom_m)\n",
    "report_series('rain_anom_3m', rain_anom_3m)\n",
    "print(\"----------------------------\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Build monthly features (inner join at month-end)\n",
    "# -----------------------------\n",
    "parts = [\n",
    "    excess_ret, midcap_mret, nifty_mret,\n",
    "    cpi_yoy_m, gdp_yoy_m, repo_m, repo_chg_bps_m,\n",
    "    rain_anom_m, rain_anom_3m\n",
    "]\n",
    "mdf = pd.concat(parts, axis=1, join='inner').sort_index()\n",
    "mdf.index.name = 'date_m'\n",
    "mdf = mdf.rename(columns={'repo': 'repo_pct'})\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Lags & target (month-ahead)\n",
    "# -----------------------------\n",
    "# Lags & target\n",
    "mdf['ret_prev_m']       = mdf['midcap_mret'].shift(1)\n",
    "mdf['rain_anom_lag']    = mdf['rain_anom_m'].shift(1)\n",
    "mdf['rain_anom_3m_lag'] = mdf['rain_anom_3m'].shift(1)\n",
    "mdf['cpi_yoy_lag']      = mdf['cpi_yoy'].shift(1)\n",
    "mdf['gdp_yoy_lag']      = mdf['gdp_yoy'].shift(1)     # add extra shift if you model publish lag\n",
    "mdf['repo_chg_lag']     = mdf['repo_chg_bps'].shift(1)\n",
    "mdf['excess_next_m']    = mdf['excess_ret'].shift(-1)\n",
    "\n",
    "mdf = mdf.dropna(how='any').copy()\n",
    "print(\"[FINAL] Monthly rows:\", len(mdf), \"|\", mdf.index.min().date(), \"→\", mdf.index.max().date())\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Save\n",
    "# -----------------------------\n",
    "out_csv  = PROC/'monthly_features.csv'\n",
    "out_parq = PROC/'monthly_features.parquet'\n",
    "\n",
    "mdf.reset_index().to_csv(out_csv, index=False)\n",
    "try:\n",
    "    engine = PARQUET_ENGINE if PARQUET_ENGINE else None\n",
    "    mdf.to_parquet(out_parq, index=True, engine=engine) if engine else mdf.to_parquet(out_parq, index=True)\n",
    "except Exception as e:\n",
    "    print(f\"[warn] Could not save parquet: {e}\")\n",
    "\n",
    "print(\"Saved:\", out_csv.resolve(), \"and\", out_parq.resolve())\n",
    "\n",
    "# Optional: quick peek\n",
    "display(mdf.head(6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
