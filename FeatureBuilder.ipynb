{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e8427b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a18325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) Imports, engine check, folders\n",
    "# =========================\n",
    "import pandas as pd, numpy as np, warnings, re\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Require a Parquet engine ---\n",
    "try:\n",
    "    import pyarrow  # noqa: F401\n",
    "    PARQUET_ENGINE = \"pyarrow\"\n",
    "except ImportError:\n",
    "    try:\n",
    "        import fastparquet  # noqa: F401\n",
    "        PARQUET_ENGINE = \"fastparquet\"\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"Parquet output is required but no engine is installed.\\n\"\n",
    "            \"Install one of:\\n\"\n",
    "            \"  pip install pyarrow\\n\"\n",
    "            \"    or\\n\"\n",
    "            \"  pip install fastparquet\"\n",
    "        ) from e\n",
    "\n",
    "RAW  = Path('./raw')            # change if your files are elsewhere\n",
    "PROC = Path('./processed'); PROC.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ef49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust column picker (case-insensitive, partial matches allowed)\n",
    "def pick(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns: return c\n",
    "    for c in df.columns:\n",
    "        for k in candidates:\n",
    "            if k.lower() in c.lower(): return c\n",
    "    return None\n",
    "\n",
    "# Smart date parser: chooses the best of multiple formats by coverage\n",
    "from dateutil import parser as _dateparser\n",
    "\n",
    "def smart_parse_dates(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str)\n",
    "    attempts = []\n",
    "    attempts.append(('infer', pd.to_datetime(s, errors='coerce')))\n",
    "    attempts.append(('dayfirst', pd.to_datetime(s, errors='coerce', dayfirst=True)))\n",
    "    fmts = ['%d-%b-%Y','%d-%b-%y','%d/%m/%Y','%m/%d/%Y','%Y-%m-%d','%b %d, %Y','%d %b %Y','%b %Y','%m-%Y']\n",
    "    for fmt in fmts:\n",
    "        attempts.append((fmt, pd.to_datetime(s, format=fmt, errors='coerce')))\n",
    "    def score(dt):\n",
    "        ok = dt.dropna()\n",
    "        if ok.empty: return (0,0,0)\n",
    "        ser = pd.Series(1, index=ok).sort_index()\n",
    "        by_year_months = ser.groupby(ser.index.year).apply(lambda x: len(pd.Index(x.index.month).unique()))\n",
    "        med_months = int(by_year_months.median()) if len(by_year_months) else 0\n",
    "        return (len(ok), med_months, len(pd.Index(ok.dt.month).unique()))\n",
    "    best_name, best_dt, best_score = max([(n,d,score(d)) for n,d in attempts], key=lambda t: t[2])\n",
    "    print(f\"[smart_parse_dates] picked: {best_name} | parsed={best_score[0]} | median months/yr={best_score[1]} | unique months={best_score[2]}\")\n",
    "    return best_dt\n",
    "\n",
    "def parse_dates_in_df(df, date_col='Date'):\n",
    "    out = df.copy()\n",
    "    out[date_col] = smart_parse_dates(out[date_col])\n",
    "    out = out.dropna(subset=[date_col]).sort_values(date_col)\n",
    "    return out\n",
    "\n",
    "def _parse_investing_prices(df, date_col='Date', price_col='Price'):\n",
    "    out = parse_dates_in_df(df, date_col=date_col)\n",
    "    out[price_col] = pd.to_numeric(out[price_col].astype(str).str.replace(',', '', regex=False), errors='coerce')\n",
    "    out = out.dropna(subset=[price_col])\n",
    "    return out[[date_col, price_col]]\n",
    "\n",
    "def investing_to_quarter_ret(df, date_col='Date', price_col='Price'):\n",
    "    df2 = _parse_investing_prices(df, date_col, price_col)\n",
    "    q_end_price = df2.set_index(date_col)[price_col].resample('Q').last()\n",
    "    return q_end_price.pct_change()\n",
    "\n",
    "def monthly_to_quarter(series, how='mean'):\n",
    "    return series.resample('Q').mean() if how=='mean' else series.resample('Q').last()\n",
    "\n",
    "def yoy_from_monthly(series):\n",
    "    return series.pct_change(12) * 100.0\n",
    "\n",
    "def check_monthly_coverage(df, date_col='Date', label='series'):\n",
    "    dates = smart_parse_dates(df[date_col]).dropna()\n",
    "    ser = pd.Series(1, index=dates).sort_index()\n",
    "    months_per_year = ser.groupby(ser.index.year).apply(lambda s: sorted(pd.Index(s.index.month).unique()))\n",
    "    print(f\"[{label}] Months present per year:\")\n",
    "    for y, months in months_per_year.items():\n",
    "        print(f\"  {y}: {months}\")\n",
    "    q_counts = ser.resample('Q').size()\n",
    "    miss = q_counts[q_counts == 0]\n",
    "    if len(miss) > 0:\n",
    "        print(f\"[{label}] ⚠ Missing {len(miss)} quarter(s) with zero rows — resample will yield NaNs.\")\n",
    "    else:\n",
    "        print(f\"[{label}] ✅ At least one row in every quarter.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "108ee50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[smart_parse_dates] picked: infer | parsed=188 | median months/yr=12 | unique months=12\n",
      "[NIFTY 50] Months present per year:\n",
      "  2010: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2011: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2012: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2013: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2014: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2015: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2016: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2017: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2018: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2019: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2020: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2021: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2022: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2023: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2024: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2025: [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[NIFTY 50] ✅ At least one row in every quarter.\n",
      "[smart_parse_dates] picked: infer | parsed=188 | median months/yr=12 | unique months=12\n",
      "[NIFTY Midcap 100] Months present per year:\n",
      "  2010: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2011: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2012: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2013: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2014: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2015: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2016: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2017: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2018: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2019: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2020: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2021: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2022: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2023: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2024: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "  2025: [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[NIFTY Midcap 100] ✅ At least one row in every quarter.\n",
      "[smart_parse_dates] picked: infer | parsed=188 | median months/yr=12 | unique months=12\n",
      "[smart_parse_dates] picked: infer | parsed=188 | median months/yr=12 | unique months=12\n",
      "Index quarterly points (excess_ret): 62\n"
     ]
    }
   ],
   "source": [
    "# Ensure files exist in ./raw\n",
    "n50_raw = pd.read_csv(RAW/'Nifty50.csv')\n",
    "mid_raw = pd.read_csv(RAW/'NIFTYMidcap100.csv')\n",
    "\n",
    "# Quick coverage QA (helps catch wrong date parsing or partial exports)\n",
    "check_monthly_coverage(n50_raw, 'Date', 'NIFTY 50')\n",
    "check_monthly_coverage(mid_raw, 'Date', 'NIFTY Midcap 100')\n",
    "\n",
    "nifty_qret  = investing_to_quarter_ret(n50_raw).rename('nifty_qret')\n",
    "midcap_qret = investing_to_quarter_ret(mid_raw).rename('midcap_qret')\n",
    "excess_ret  = (midcap_qret - nifty_qret).rename('excess_ret')\n",
    "\n",
    "print(\"Index quarterly points (excess_ret):\", excess_ret.dropna().shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71cf198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[smart_parse_dates] picked: infer | parsed=150 | median months/yr=12 | unique months=12\n",
      "CPI monthly points: 150 | CPI YoY monthly (non-NaN): 138\n",
      "CPI quarterly points: 46\n"
     ]
    }
   ],
   "source": [
    "# CPI monthly to YoY% to quarterly mean\n",
    "cpi = pd.read_csv(RAW/'CPI_Monthly_Jan_2013_to_Jun_2025.csv')  # <- file name you uploaded\n",
    "c_date = pick(cpi, ['DATE','Date','Month','Period'])           # will choose 'DATE'\n",
    "c_val  = pick(cpi, ['CPI','Index','Value','CPI_COMBINED_BASE2012_100'])  # will choose 'CPI_COMBINED_BASE2012_100'\n",
    "\n",
    "cpi = parse_dates_in_df(cpi, c_date)   # smart date parser picked \"infer\" in my check\n",
    "cpi_m = cpi.set_index(c_date)[c_val].astype(float).rename('cpi_index')\n",
    "\n",
    "# YoY requires 12 months of history, so the first 12 rows will be NaN (expected)\n",
    "cpi_yoy_m = yoy_from_monthly(cpi_m).rename('cpi_yoy_m')\n",
    "\n",
    "# Aggregate to quarterly (mean of three months)\n",
    "cpi_yoy_q = monthly_to_quarter(cpi_yoy_m, how='mean').rename('cpi_yoy')\n",
    "print(\"CPI monthly points:\", cpi_m.shape[0], \"| CPI YoY monthly (non-NaN):\", cpi_yoy_m.dropna().shape[0])\n",
    "print(\"CPI quarterly points:\", cpi_yoy_q.dropna().shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8fbf1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique 'quarter' samples: ['2012.0-Q1' '2012.0-Q2' '2012.0-Q3' '2012.0-Q4' '2013.0-Q1' '2013.0-Q2'\n",
      " '2013.0-Q3' '2013.0-Q4']\n",
      "Normalized samples: ['2012-Q1' '2012-Q2' '2012-Q3' '2012-Q4' '2013-Q1' '2013-Q2' '2013-Q3'\n",
      " '2013-Q4']\n",
      "GDP quarterly points: 50\n",
      "GDP range: 2013-03-31 → 2025-06-30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "__q\n",
       "2013-03-31    4.296010\n",
       "2013-06-30    6.447099\n",
       "2013-09-30    7.337749\n",
       "2013-12-31    6.534983\n",
       "Freq: QE-DEC, Name: gdp_yoy, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- GDP quarterly loader (fixes '2012.0-Q1' etc.) ---\n",
    "\n",
    "import re\n",
    "from pandas.tseries.offsets import QuarterEnd\n",
    "\n",
    "gdp_raw = pd.read_csv(RAW/'GDP_Quarterly_2010_2025.csv').copy()\n",
    "\n",
    "# 1) Clean numeric column\n",
    "if 'gdp' not in gdp_raw.columns:\n",
    "    raise ValueError(\"Expected a 'gdp' column in GDP_Quarterly_2010_2025.csv\")\n",
    "gdp_raw['gdp_clean'] = pd.to_numeric(\n",
    "    gdp_raw['gdp'].astype(str).str.replace(',', '', regex=False),\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# 2) Normalize quarter strings (strip the '.0' before '-Q')\n",
    "if 'quarter' not in gdp_raw.columns:\n",
    "    raise ValueError(\"Expected a 'quarter' column in GDP_Quarterly_2010_2025.csv\")\n",
    "\n",
    "gdp_raw['quarter_str'] = gdp_raw['quarter'].astype(str).str.replace('.0', '', regex=False)\n",
    "\n",
    "# 3) Extract YYYY and Qn from anything like '2012-Q1', '2012Q1', 'Q1 2012.0', etc.\n",
    "m = gdp_raw['quarter_str'].str.extract(r'(?P<y>\\d{4}).*?Q(?P<q>[1-4])')\n",
    "qstr = (m['y'].fillna('') + 'Q' + m['q'].fillna(''))\n",
    "valid = qstr.str.match(r'^\\d{4}Q[1-4]$')\n",
    "\n",
    "gdp_norm = gdp_raw.loc[valid].copy()\n",
    "gdp_norm['__q'] = pd.PeriodIndex(qstr[valid], freq='Q').to_timestamp(how='end')\n",
    "\n",
    "# 4) Build a clean quarterly level series (last obs per quarter if duplicates)\n",
    "gdp_q_level = (gdp_norm\n",
    "               .dropna(subset=['__q','gdp_clean'])\n",
    "               .set_index('__q')\n",
    "               .sort_index()\n",
    "               .groupby(pd.Grouper(freq='Q'))['gdp_clean']\n",
    "               .last())\n",
    "\n",
    "# 5) Compute YoY from levels (4-quarter change * 100)\n",
    "gdp_q = gdp_q_level.pct_change(4).mul(100.0).rename('gdp_yoy').dropna()\n",
    "\n",
    "# 6) Diagnostics\n",
    "print(\"Unique 'quarter' samples:\", gdp_raw['quarter'].astype(str).unique()[:8])\n",
    "print(\"Normalized samples:\", gdp_norm['quarter_str'].astype(str).unique()[:8])\n",
    "print(\"GDP quarterly points:\", gdp_q.shape[0])\n",
    "if not gdp_q.empty:\n",
    "    print(\"GDP range:\", gdp_q.index.min().date(), \"→\", gdp_q.index.max().date())\n",
    "display(gdp_q.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc385914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[smart_parse_dates] picked: infer | parsed=188 | median months/yr=12 | unique months=12\n",
      "[Repo] months covered: 12 median months/yr; quarters: 63 (2010-03-31 → 2025-09-30)\n",
      "2024-09-30    6.50\n",
      "2024-12-31    6.50\n",
      "2025-03-31    6.25\n",
      "2025-06-30    5.50\n",
      "2025-09-30    5.50\n",
      "Freq: QE-DEC, Name: repo, dtype: float64\n",
      "2024-09-30     0.0\n",
      "2024-12-31     0.0\n",
      "2025-03-31   -25.0\n",
      "2025-06-30   -75.0\n",
      "2025-09-30     0.0\n",
      "Freq: QE-DEC, Name: repo_chg_bps, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_repo_monthly_to_quarter(path: Path):\n",
    "    raw = pd.read_csv(path)\n",
    "\n",
    "    # 1) find date & rate columns\n",
    "    dcol = pick(raw, ['DATE','Date','Month','Period'])\n",
    "    vcol = pick(raw, ['REPO_RATE_PERCENT','Repo','Rate','Policy Rate','REPO'])\n",
    "    if dcol is None or vcol is None:\n",
    "        raise ValueError(\"Repo CSV must have a date column (DATE/Month/Period) and a value column (Repo/Rate/REPO_RATE_PERCENT).\")\n",
    "\n",
    "    # 2) parse dates & clean numeric\n",
    "    df = parse_dates_in_df(raw, dcol)\n",
    "    val = (df[vcol].astype(str)\n",
    "                  .str.replace(',', '', regex=False)\n",
    "                  .str.replace('%', '', regex=False)\n",
    "                  .str.strip())\n",
    "    val = pd.to_numeric(val, errors='coerce')\n",
    "\n",
    "    # 3) detect scaling (percent vs fraction)\n",
    "    # typical RBI repo is ~4–10; if values look like 0.04–0.10, scale by 100\n",
    "    median_abs = val.dropna().abs().median()\n",
    "    if pd.notna(median_abs) and median_abs < 1:  # e.g., 0.065\n",
    "        val = val * 100.0\n",
    "\n",
    "    df = df.assign(val=val).dropna(subset=['val'])\n",
    "\n",
    "    # 4) normalize to month-end stamps, forward-fill missing months\n",
    "    # (so quarter-end 'last' is well-defined even if a month is missing)\n",
    "    m_end = (df.set_index(dcol)['val']\n",
    "               .to_period('M').to_timestamp('M')\n",
    "               .sort_index())\n",
    "\n",
    "    # create a complete monthly index, forward-fill within series\n",
    "    full_m = pd.date_range(m_end.index.min(), m_end.index.max(), freq='M')\n",
    "    m_end = m_end.reindex(full_m).ffill()\n",
    "\n",
    "    # 5) quarter-end level & QoQ change in bps\n",
    "    repo_q = m_end.resample('Q').last().rename('repo')                 # % at quarter-end\n",
    "    repo_chg_bps = (repo_q.diff() * 100.0).rename('repo_chg_bps')      # Δ in basis points\n",
    "\n",
    "    # 6) coverage diagnostics\n",
    "    by_year_months = pd.Series(1, index=full_m).groupby(pd.Grouper(freq='Y')).size()\n",
    "    print(f\"[Repo] months covered: {int(by_year_months.median())} median months/yr; \"\n",
    "          f\"quarters: {repo_q.shape[0]} ({repo_q.index.min().date()} → {repo_q.index.max().date()})\")\n",
    "\n",
    "    return repo_q, repo_chg_bps\n",
    "\n",
    "# ---- run the loader\n",
    "repo_q, repo_chg_bps = load_repo_monthly_to_quarter(RAW/'Repo_Rate_Monthly_2010_2025.csv')\n",
    "\n",
    "# quick peek\n",
    "print(repo_q.tail())\n",
    "print(repo_chg_bps.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99452c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rainfall seasonal years covered: [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025]\n",
      "Rainfall quarterly points: 53 | Range: 2012-09-30 → 2025-09-30\n"
     ]
    }
   ],
   "source": [
    "# ---- Rainfall seasonal anomaly from your monthly file (2012–2025) ----\n",
    "# File columns (confirmed): year, month (\"Jan\"..\"Dec\"), rainfall_mm, good_rainfall_mm, anomaly_mm\n",
    "\n",
    "rain = pd.read_csv(RAW/'AnnualRainfall_with_Good_and_Anomaly_2012_2025.csv')\n",
    "\n",
    "# Flexible column picks (if you already have pick(), you can use it; here we reference directly)\n",
    "ycol, mcol = 'year', 'month'\n",
    "obs_col, norm_col, mm_anom_col = 'rainfall_mm', 'good_rainfall_mm', 'anomaly_mm'  # we will *not* use anomaly_mm\n",
    "\n",
    "# Normalize month text -> month number (1..12)\n",
    "mmap = {m[:3].lower(): i for i, m in enumerate(\n",
    "    ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'], start=1)}\n",
    "rain['mon'] = rain[mcol].astype(str).str[:3].str.lower().map(mmap).astype(int)\n",
    "\n",
    "# Keep Southwest monsoon months: Jun(6)–Sep(9)\n",
    "mons = rain[rain['mon'].between(6, 9)].copy()\n",
    "\n",
    "# --- QA 1: ensure each monsoon season has its 4 months ---\n",
    "miss = (mons.groupby(ycol)['mon']\n",
    "            .nunique()\n",
    "            .rename('monsoon_months')\n",
    "            .reset_index())\n",
    "bad = miss[miss['monsoon_months'] < 4]\n",
    "if not bad.empty:\n",
    "    print(\"⚠ Monsoon months missing in years:\", bad[ycol].tolist())\n",
    "\n",
    "# Aggregate to seasonal totals per year (use *observed* and *normal*; ignore anomaly_mm to avoid sign confusion)\n",
    "grp = mons.groupby(ycol, as_index=False).agg(\n",
    "    obs_mm  = (obs_col,  'sum'),\n",
    "    norm_mm = (norm_col, 'sum'),\n",
    "    # anom_mm_sum = (mm_anom_col, 'sum')  # optional: cross-check below\n",
    ")\n",
    "\n",
    "# IMD-style anomaly%: (observed - normal)/normal * 100  (positive = above normal)\n",
    "grp['rain_anom_pct'] = (grp['obs_mm'] - grp['norm_mm']) / grp['norm_mm'] * 100.0\n",
    "\n",
    "# Optional cross-check using your provided anomaly_mm (remember: good - observed)\n",
    "# If you want to verify signs, uncomment:\n",
    "# check = mons.groupby(ycol, as_index=False).agg(anom_mm_sum=(mm_anom_col, 'sum'),\n",
    "#                                                norm_mm_sum=(norm_col, 'sum'))\n",
    "# check['alt_pct_from_file'] = (-check['anom_mm_sum'] / check['norm_mm_sum']) * 100.0\n",
    "# print(check[[ycol,'alt_pct_from_file']].head())\n",
    "\n",
    "# Stamp each year's anomaly at Sep-30; forward-fill within the same year → quarterly series\n",
    "rain_idx = pd.to_datetime(grp[ycol].astype(int).astype(str) + '-09-30')\n",
    "rain_q = (pd.Series(grp['rain_anom_pct'].values, index=rain_idx)\n",
    "            .resample('Q').ffill()\n",
    "            .rename('rain_anom'))\n",
    "\n",
    "print(\"Rainfall seasonal years covered:\", grp[ycol].tolist())\n",
    "print(\"Rainfall quarterly points:\", rain_q.shape[0], \"| Range:\", rain_q.index.min().date(), \"→\", rain_q.index.max().date())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b155a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final quarterly rows: 44 | Range: 2014-06-30 → 2025-03-31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>midcap_qret</th>\n",
       "      <th>nifty_qret</th>\n",
       "      <th>excess_ret</th>\n",
       "      <th>rain_anom</th>\n",
       "      <th>cpi_yoy</th>\n",
       "      <th>gdp_yoy</th>\n",
       "      <th>repo_chg_bps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-06-30</th>\n",
       "      <td>0.288472</td>\n",
       "      <td>0.135311</td>\n",
       "      <td>0.153161</td>\n",
       "      <td>69.879102</td>\n",
       "      <td>7.859486</td>\n",
       "      <td>8.023963</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-30</th>\n",
       "      <td>0.028963</td>\n",
       "      <td>0.046437</td>\n",
       "      <td>-0.017474</td>\n",
       "      <td>-7.449626</td>\n",
       "      <td>6.681568</td>\n",
       "      <td>8.704109</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31</th>\n",
       "      <td>0.102077</td>\n",
       "      <td>0.039913</td>\n",
       "      <td>0.062164</td>\n",
       "      <td>-7.449626</td>\n",
       "      <td>4.054538</td>\n",
       "      <td>5.922736</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>0.033169</td>\n",
       "      <td>0.025149</td>\n",
       "      <td>0.008021</td>\n",
       "      <td>-7.449626</td>\n",
       "      <td>5.272440</td>\n",
       "      <td>7.112080</td>\n",
       "      <td>-50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>0.000646</td>\n",
       "      <td>-0.014427</td>\n",
       "      <td>0.015073</td>\n",
       "      <td>-7.449626</td>\n",
       "      <td>5.090809</td>\n",
       "      <td>7.592544</td>\n",
       "      <td>-25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>-0.001933</td>\n",
       "      <td>-0.050140</td>\n",
       "      <td>0.048207</td>\n",
       "      <td>-6.171560</td>\n",
       "      <td>3.948304</td>\n",
       "      <td>8.033806</td>\n",
       "      <td>-50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            midcap_qret  nifty_qret  excess_ret  rain_anom   cpi_yoy  \\\n",
       "2014-06-30     0.288472    0.135311    0.153161  69.879102  7.859486   \n",
       "2014-09-30     0.028963    0.046437   -0.017474  -7.449626  6.681568   \n",
       "2014-12-31     0.102077    0.039913    0.062164  -7.449626  4.054538   \n",
       "2015-03-31     0.033169    0.025149    0.008021  -7.449626  5.272440   \n",
       "2015-06-30     0.000646   -0.014427    0.015073  -7.449626  5.090809   \n",
       "2015-09-30    -0.001933   -0.050140    0.048207  -6.171560  3.948304   \n",
       "\n",
       "             gdp_yoy  repo_chg_bps  \n",
       "2014-06-30  8.023963           0.0  \n",
       "2014-09-30  8.704109           0.0  \n",
       "2014-12-31  5.922736           0.0  \n",
       "2015-03-31  7.112080         -50.0  \n",
       "2015-06-30  7.592544         -25.0  \n",
       "2015-09-30  8.033806         -50.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Build the modeling DataFrame on the shared quarterly intersection\n",
    "parts = [excess_ret, midcap_qret, nifty_qret, rain_q, cpi_yoy_q, gdp_q, repo_chg_bps]\n",
    "qdf = pd.concat(parts, axis=1, join='inner')  # inner = strict intersection of all indices\n",
    "qdf.index = qdf.index.to_period('Q').to_timestamp('Q')\n",
    "# 2) Create lags (t-1) and the prediction target (t+1)\n",
    "qdf['ret_prev_q']    = qdf['midcap_qret'].shift(1)\n",
    "qdf['rain_anom_lag'] = qdf['rain_anom'].shift(1)\n",
    "qdf['cpi_yoy_lag']   = qdf['cpi_yoy'].shift(1)\n",
    "qdf['gdp_yoy_lag']   = qdf['gdp_yoy'].shift(1)\n",
    "qdf['repo_chg_lag']  = qdf['repo_chg_bps'].shift(1)\n",
    "qdf['excess_next_q'] = qdf['excess_ret'].shift(-1)\n",
    "\n",
    "# 3) Clean rows created by shifting and print a quick QA\n",
    "qdf = qdf.dropna(how=\"any\").copy()\n",
    "print(\"Final quarterly rows:\", len(qdf), \"| Range:\", qdf.index.min().date(), \"→\", qdf.index.max().date())\n",
    "qdf[['midcap_qret','nifty_qret','excess_ret','rain_anom','cpi_yoy','gdp_yoy','repo_chg_bps']].head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdf_out = qdf.copy()\n",
    "qdf_out['excess_next_q'] = qdf_out['excess_ret'].shift(-1)\n",
    "qdf_out['ret_prev_q']    = qdf_out['midcap_qret'].shift( -1 )  \n",
    "qdf_final = qdf.copy().reset_index().rename(columns={'index':'date_q'})\n",
    "qdf_final.to_csv(PROC/'quarterly_features.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
