{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93987d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "DateParseError",
     "evalue": "Unknown datetime string format, unable to parse: 2012.0Q1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mperiod.pyx:1589\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.period._extract_ordinal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'ordinal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDateParseError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 162\u001b[0m\n\u001b[0;32m    160\u001b[0m         q \u001b[38;5;241m=\u001b[39m g[qcol]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    161\u001b[0m         q_norm \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-Q\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 162\u001b[0m         g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mPeriodIndex(q_norm, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto_timestamp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    163\u001b[0m         gdp \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_date\u001b[39m\u001b[38;5;124m'\u001b[39m); c_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_date\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Create GDP YoY series\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\period.py:314\u001b[0m, in \u001b[0;36mPeriodIndex.__new__\u001b[1;34m(cls, data, ordinal, freq, dtype, copy, name, **fields)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot pass both data and ordinal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# don't pass copy here, since we copy later.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m         data \u001b[38;5;241m=\u001b[39m period_array(data\u001b[38;5;241m=\u001b[39mdata, freq\u001b[38;5;241m=\u001b[39mfreq)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m    317\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\period.py:1108\u001b[0m, in \u001b[0;36mperiod_array\u001b[1;34m(data, freq, copy)\u001b[0m\n\u001b[0;32m   1106\u001b[0m     freq \u001b[38;5;241m=\u001b[39m libperiod\u001b[38;5;241m.\u001b[39mextract_freq(data)\n\u001b[0;32m   1107\u001b[0m dtype \u001b[38;5;241m=\u001b[39m PeriodDtype(freq)\n\u001b[1;32m-> 1108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PeriodArray\u001b[38;5;241m.\u001b[39m_from_sequence(data, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\period.py:304\u001b[0m, in \u001b[0;36mPeriodArray._from_sequence\u001b[1;34m(cls, scalars, dtype, copy)\u001b[0m\n\u001b[0;32m    301\u001b[0m periods \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(scalars, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m    303\u001b[0m freq \u001b[38;5;241m=\u001b[39m freq \u001b[38;5;129;01mor\u001b[39;00m libperiod\u001b[38;5;241m.\u001b[39mextract_freq(periods)\n\u001b[1;32m--> 304\u001b[0m ordinals \u001b[38;5;241m=\u001b[39m libperiod\u001b[38;5;241m.\u001b[39mextract_ordinals(periods, freq)\n\u001b[0;32m    305\u001b[0m dtype \u001b[38;5;241m=\u001b[39m PeriodDtype(freq)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(ordinals, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mperiod.pyx:1563\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.period.extract_ordinals\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mperiod.pyx:1598\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.period._extract_ordinal\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mperiod.pyx:2782\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.period.Period.__new__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mperiod.pyx:2774\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.period.Period.__new__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsing.pyx:442\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string_with_reso\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsing.pyx:666\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDateParseError\u001b[0m: Unknown datetime string format, unable to parse: 2012.0Q1"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) Imports, engine check, folders\n",
    "# =========================\n",
    "import pandas as pd, numpy as np, warnings, re\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Require a Parquet engine ---\n",
    "try:\n",
    "    import pyarrow  # noqa: F401\n",
    "    PARQUET_ENGINE = \"pyarrow\"\n",
    "except ImportError:\n",
    "    try:\n",
    "        import fastparquet  # noqa: F401\n",
    "        PARQUET_ENGINE = \"fastparquet\"\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"Parquet output is required but no engine is installed.\\n\"\n",
    "            \"Install one of:\\n\"\n",
    "            \"  pip install pyarrow\\n\"\n",
    "            \"    or\\n\"\n",
    "            \"  pip install fastparquet\"\n",
    "        ) from e\n",
    "\n",
    "RAW  = Path('./raw')            # change if your files are elsewhere\n",
    "PROC = Path('./processed'); PROC.mkdir(exist_ok=True)\n",
    "\n",
    "# Filenames (exact)\n",
    "f_rain = RAW/'AnnualRainfall_with_Good_and_Anomaly_2012_2025.csv'\n",
    "f_cpi  = RAW/'CPI_Monthly_Jan_2013_to_Jun_2025.csv'\n",
    "f_gdp  = RAW/'GDP_Quarterly_2010_2025.csv'\n",
    "f_n50  = RAW/'Nifty50.csv'\n",
    "f_mid  = RAW/'NIFTYMidcap100.csv'\n",
    "f_repo = RAW/'Repo_Rate_Monthly_2010_2025.csv'\n",
    "\n",
    "# =========================\n",
    "# 1) Helpers\n",
    "# =========================\n",
    "def _first_col(df, candidates):\n",
    "    # exact match first\n",
    "    for c in candidates:\n",
    "        if c in df.columns: return c\n",
    "    # relaxed match\n",
    "    for c in df.columns:\n",
    "        for k in candidates:\n",
    "            if k.lower() in c.lower(): return c\n",
    "    return None\n",
    "\n",
    "def _ensure_datetime(df, col):\n",
    "    if not np.issubdtype(df[col].dtype, np.datetime64):\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def daily_to_quarter_ret(df, date_col, price_col):\n",
    "    df = df[[date_col, price_col]].dropna().sort_values(date_col)\n",
    "    df = _ensure_datetime(df, date_col)\n",
    "    df[price_col] = (\n",
    "        df[price_col].astype(str)\n",
    "        .str.replace(',', '', regex=False)\n",
    "        .str.replace('(', '-', regex=False)\n",
    "        .str.replace(')', '', regex=False)\n",
    "        .str.strip()\n",
    "        .replace({'': np.nan, 'None': np.nan})\n",
    "        .astype(float)\n",
    "    )\n",
    "    df = df.set_index(date_col)\n",
    "    q_price = df[price_col].resample('Q').last()\n",
    "    q_ret   = q_price.pct_change().rename(price_col + '_qret')\n",
    "    return q_ret\n",
    "\n",
    "def monthly_to_quarter(series_monthly, how='mean'):\n",
    "    return series_monthly.resample('Q').mean() if how == 'mean' else series_monthly.resample('Q').last()\n",
    "\n",
    "def yoy_from_monthly(series):\n",
    "    return series.pct_change(12) * 100.0\n",
    "\n",
    "def to_quarter_end_unique(s, agg='last'):\n",
    "    \"\"\"Snap any datetime-like index to quarter-end and collapse duplicate quarters.\"\"\"\n",
    "    s = s.copy()\n",
    "    if not isinstance(s.index, pd.DatetimeIndex):\n",
    "        s.index = pd.to_datetime(s.index, errors='coerce')\n",
    "    s = s[~s.index.isna()]\n",
    "    s.index = s.index.to_period('Q').to_timestamp('Q')\n",
    "    s = s.groupby(level=0).mean() if agg == 'mean' else s.groupby(level=0).last()\n",
    "    return s.sort_index()\n",
    "\n",
    "# =========================\n",
    "# 2) Index prices -> quarterly returns\n",
    "# =========================\n",
    "# NIFTY 50\n",
    "n50 = pd.read_csv(f_n50)\n",
    "c_date = _first_col(n50, ['Date','date','Time'])\n",
    "c_px   = _first_col(n50, ['Close','Adj Close','Price','Close Price','Value'])\n",
    "if c_date is None or c_px is None:\n",
    "    raise ValueError(\"Nifty50.csv columns not recognized. Expected Date & Close/Price-like column.\")\n",
    "n50_qret = daily_to_quarter_ret(n50, c_date, c_px).rename('nifty_qret')\n",
    "n50_qret = to_quarter_end_unique(n50_qret)\n",
    "\n",
    "# MIDCAP 100\n",
    "mid = pd.read_csv(f_mid)\n",
    "c_date_m = _first_col(mid, ['Date','date','Time'])\n",
    "c_px_m   = _first_col(mid, ['Close','Adj Close','Price','Close Price','Value'])\n",
    "if c_date_m is None or c_px_m is None:\n",
    "    raise ValueError(\"NIFTYMidcap100.csv columns not recognized. Expected Date & Close/Price-like column.\")\n",
    "mid_qret = daily_to_quarter_ret(mid, c_date_m, c_px_m).rename('midcap_qret')\n",
    "mid_qret = to_quarter_end_unique(mid_qret)\n",
    "\n",
    "# Excess return for quarter t\n",
    "excess_q = (mid_qret - n50_qret).rename('excess_ret')\n",
    "excess_q = to_quarter_end_unique(excess_q)\n",
    "\n",
    "# =========================\n",
    "# 3) Rainfall anomaly (monthly -> annual avg -> quarterly)\n",
    "# =========================\n",
    "rain = pd.read_csv(f_rain)\n",
    "c_year = _first_col(rain, ['Year','year','YYYY'])\n",
    "c_anom = _first_col(rain, ['Anomaly','Anomaly_%','Anomaly %','Rainfall Anomaly','anomaly_mm'])\n",
    "if c_year is None or c_anom is None:\n",
    "    raise ValueError(\"Rainfall file must contain Year & an anomaly column (e.g., anomaly_mm).\")\n",
    "rain = rain[[c_year, c_anom]].dropna()\n",
    "rain.columns = ['Year','rain_anom']     # units: mm\n",
    "# If multiple rows per year (monthly), collapse to the annual mean anomaly:\n",
    "if rain.groupby('Year').size().max() > 1:\n",
    "    rain = rain.groupby('Year', as_index=False)['rain_anom'].mean()\n",
    "# Anchor at Sep-30 (monsoon year) then quarter-fill:\n",
    "rain_idx = pd.to_datetime(rain['Year'].astype(int).astype(str) + '-09-30')\n",
    "rain_q = pd.Series(rain['rain_anom'].values, index=rain_idx).resample('Q').ffill().rename('rain_anom')\n",
    "rain_q = to_quarter_end_unique(rain_q)\n",
    "\n",
    "# =========================\n",
    "# 4) CPI monthly -> YoY -> quarterly mean\n",
    "# =========================\n",
    "cpi = pd.read_csv(f_cpi)\n",
    "c_date = _first_col(cpi, ['Date','date','Month','month','Period'])\n",
    "c_val  = _first_col(cpi, ['Index','CPI','Value','CPI Index','cpi'])\n",
    "if c_date is None or c_val is None:\n",
    "    raise ValueError(\"CPI file must have Date & CPI value columns.\")\n",
    "cpi = _ensure_datetime(cpi, c_date).sort_values(c_date)\n",
    "cpi_m = cpi.set_index(c_date)[c_val].astype(float).rename('cpi_index')\n",
    "cpi_yoy_m = yoy_from_monthly(cpi_m).rename('cpi_yoy_m')\n",
    "cpi_yoy_q = monthly_to_quarter(cpi_yoy_m, how='mean').rename('cpi_yoy')\n",
    "cpi_yoy_q = to_quarter_end_unique(cpi_yoy_q, agg='mean')\n",
    "\n",
    "# =========================\n",
    "# 5) GDP quarterly (YoY % preferred)\n",
    "# =========================\n",
    "gdp = pd.read_csv(f_gdp)\n",
    "c_date = _first_col(gdp, ['Date','date','Quarter','quarter','Period'])\n",
    "c_yoy  = _first_col(gdp, ['YoY','GDP_YoY','gdp_yoy','Growth','Growth YoY'])\n",
    "c_lvl  = _first_col(gdp, ['GDP','Value','gdp_sa','gdp','real_gdp_inr_millions','real_gdp_inr_cr'])\n",
    "if c_date is None:\n",
    "    raise ValueError(\"GDP csv must have a Date/Quarter column.\")\n",
    "gdp = _ensure_datetime(gdp, c_date).sort_values(c_date)\n",
    "\n",
    "# If parsing failed, handle 'YYYY-Qn' style:\n",
    "if gdp[c_date].isna().all():\n",
    "    qcol = _first_col(pd.read_csv(f_gdp, nrows=5), ['Quarter','quarter','Period'])\n",
    "    if qcol:\n",
    "        g = pd.read_csv(f_gdp)\n",
    "        q = g[qcol].astype(str).str.upper().str.replace(' ', '')\n",
    "        q_norm = q.str.replace('-Q', 'Q', regex=False)\n",
    "        g['_date'] = pd.PeriodIndex(q_norm, freq='Q').to_timestamp('Q')\n",
    "        gdp = g.sort_values('_date'); c_date = '_date'\n",
    "\n",
    "# Create GDP YoY series\n",
    "if c_yoy is not None:\n",
    "    gdp_q = gdp.set_index(c_date)[c_yoy].astype(float).rename('gdp_yoy')\n",
    "elif c_lvl is not None:\n",
    "    gdp_q = (gdp.set_index(c_date)[c_lvl].astype(float).pct_change(4) * 100.0).rename('gdp_yoy')\n",
    "else:\n",
    "    raise ValueError(\"GDP file must have either a YoY% column or a level column to compute YoY.\")\n",
    "\n",
    "gdp_q = to_quarter_end_unique(gdp_q)\n",
    "\n",
    "# =========================\n",
    "# 6) Repo rate monthly -> quarterly Δ (bps)\n",
    "# =========================\n",
    "if f_repo.exists():\n",
    "    repo = pd.read_csv(f_repo)\n",
    "    c_date = _first_col(repo, ['Date','date','Period'])\n",
    "    c_rate = _first_col(repo, ['Repo','Rate','Repo Rate','Policy Rate'])\n",
    "    repo = _ensure_datetime(repo, c_date).sort_values(c_date)\n",
    "    repo_q = repo.set_index(c_date)[c_rate].astype(float).resample('Q').last().rename('repo')\n",
    "    repo_chg_q = (repo_q.diff()*100).rename('repo_chg_bps')  # % → bps delta\n",
    "    repo_chg_q = to_quarter_end_unique(repo_chg_q)\n",
    "else:\n",
    "    repo_chg_q = pd.Series(dtype=float, name='repo_chg_bps')\n",
    "\n",
    "# =========================\n",
    "# 7) Align to common quarterly index & build features\n",
    "# =========================\n",
    "qdf = pd.concat(\n",
    "    [excess_q.rename('excess_ret'),\n",
    "     mid_qret.rename('midcap_ret'),\n",
    "     n50_qret.rename('nifty_ret'),\n",
    "     rain_q, cpi_yoy_q, gdp_q, repo_chg_q],\n",
    "    axis=1\n",
    ").dropna(how='any')\n",
    "\n",
    "# Features (lagged 1Q):\n",
    "qdf['ret_prev_q']    = qdf['midcap_ret'].shift(1)\n",
    "qdf['rain_anom_lag'] = qdf['rain_anom'].shift(1)          # mm\n",
    "qdf['cpi_yoy_lag']   = qdf['cpi_yoy'].shift(1)            # %\n",
    "qdf['gdp_yoy_lag']   = qdf['gdp_yoy'].shift(1)            # %\n",
    "if 'repo_chg_bps' in qdf.columns:\n",
    "    qdf['repo_chg_lag'] = qdf['repo_chg_bps'].shift(1)    # bps\n",
    "\n",
    "# Target (excess return next quarter)\n",
    "qdf['excess_next_q'] = qdf['excess_ret'].shift(-1)\n",
    "\n",
    "# Policy interaction (if repo present)\n",
    "if 'repo_chg_lag' in qdf.columns:\n",
    "    qdf['rain_repo_int'] = qdf['rain_anom_lag'] * qdf['repo_chg_lag']\n",
    "\n",
    "# Clean NA rows from shifts\n",
    "qdf = qdf.dropna().copy()\n",
    "\n",
    "# =========================\n",
    "# 8) Sanity checks & save\n",
    "# =========================\n",
    "print(\"Quarterly rows:\", len(qdf), \"| Range:\", qdf.index.min().date(), \"→\", qdf.index.max().date())\n",
    "print(qdf.filter(['excess_ret','excess_next_q','ret_prev_q','rain_anom_lag','cpi_yoy_lag','gdp_yoy_lag']).head(8))\n",
    "\n",
    "# Save both\n",
    "qdf.to_parquet(PROC/'quarterly_features.parquet', engine=PARQUET_ENGINE)\n",
    "qdf.to_csv(PROC/'quarterly_features.csv')\n",
    "\n",
    "# Small dictionary describing columns (units clarified)\n",
    "data_dict = {\n",
    "  'excess_ret': 'Midcap100_qret - Nifty50_qret (quarter t)',\n",
    "  'excess_next_q': 'Excess return in quarter t+1 (prediction target)',\n",
    "  'ret_prev_q': 'Midcap100 return in t-1',\n",
    "  'rain_anom_lag': 'All-India rainfall anomaly (t-1, mm)',\n",
    "  'cpi_yoy_lag': 'CPI YoY (t-1, %)',\n",
    "  'gdp_yoy_lag': 'Real GDP YoY (t-1, %)',\n",
    "  'repo_chg_lag': 'Repo change in bps (t-1) [if provided]',\n",
    "  'rain_repo_int': 'Interaction: rain_anom_lag × repo_chg_lag [if provided]'\n",
    "}\n",
    "pd.Series(data_dict).to_csv(PROC/'data_dictionary.csv')\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" -\", (PROC/'quarterly_features.parquet').resolve())\n",
    "print(\" -\", (PROC/'quarterly_features.csv').resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55314bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Baseline vs Enriched (RQ-1) ---\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "X_base = qdf[['ret_prev_q']]\n",
    "y      = qdf['excess_next_q']\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "def cv_pred(maker, X):\n",
    "    y_true, y_pred = [], []\n",
    "    for tr, te in tscv.split(X):\n",
    "        mdl = maker(); mdl.fit(X.iloc[tr], y.iloc[tr])\n",
    "        y_pred.extend(mdl.predict(X.iloc[te])); y_true.extend(y.iloc[te])\n",
    "    return np.array(y_true), np.array(y_pred)\n",
    "\n",
    "y_t, y_p = cv_pred(lambda: ElasticNetCV(cv=3), X_base)\n",
    "print(\"Baseline  R²:\", r2_score(y_t, y_p), \"MAE:\", mean_absolute_error(y_t, y_p))\n",
    "\n",
    "feat_cols = ['ret_prev_q','rain_anom_lag','cpi_yoy_lag','gdp_yoy_lag']\n",
    "if 'repo_chg_lag' in qdf.columns: feat_cols.append('repo_chg_lag')\n",
    "X_en = qdf[feat_cols]\n",
    "\n",
    "y_t2, y_p2 = cv_pred(lambda: LGBMRegressor(n_estimators=300, learning_rate=0.05, max_depth=3), X_en)\n",
    "print(\"Enriched  R²:\", r2_score(y_t2, y_p2), \"MAE:\", mean_absolute_error(y_t2, y_p2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f95c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RQ-2 Good vs Poor monsoon ---\n",
    "from scipy.stats import ttest_ind, ks_2samp\n",
    "good = qdf.loc[qdf['rain_anom_lag'] >= 4,  'excess_next_q']\n",
    "poor = qdf.loc[qdf['rain_anom_lag'] <= -4, 'excess_next_q']\n",
    "print(\"t-test:\", ttest_ind(good, poor, equal_var=False))\n",
    "print(\"KS    :\", ks_2samp(good, poor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RQ-3 Rain → GDP lead-lag ---\n",
    "import statsmodels.api as sm\n",
    "# align GDP on same index\n",
    "gdp_y = qdf['gdp_yoy_lag'].shift(-1)  # GDP_{t+1}\n",
    "X = sm.add_constant(qdf['rain_anom_lag'])\n",
    "res = sm.OLS(gdp_y.dropna(), X.loc[gdp_y.dropna().index]).fit()\n",
    "print(res.summary())\n",
    "\n",
    "# engineered feature & re-run enriched model with gdp_pred_from_rain\n",
    "qdf['gdp_pred_from_rain'] = (res.params['const'] + res.params['rain_anom_lag']*qdf['rain_anom_lag'])\n",
    "feat_cols2 = feat_cols + ['gdp_pred_from_rain']\n",
    "y_t3, y_p3 = cv_pred(lambda: LGBMRegressor(n_estimators=300, learning_rate=0.05, max_depth=3), qdf[feat_cols2])\n",
    "print(\"Enriched+Rain→GDP  R²:\", r2_score(y_t3, y_p3), \"MAE:\", mean_absolute_error(y_t3, y_p3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69ac813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RQ-4 Policy Amplifier (if repo available) ---\n",
    "if 'repo_chg_lag' in qdf.columns:\n",
    "    qdf['rain_repo_int'] = qdf['rain_anom_lag'] * qdf['repo_chg_lag']\n",
    "    Xint = sm.add_constant(qdf[['rain_anom_lag','repo_chg_lag','rain_repo_int']])\n",
    "    res_int = sm.OLS(qdf['excess_next_q'], Xint).fit()\n",
    "    print(res_int.summary())\n",
    "\n",
    "    feat_cols3 = feat_cols2 + ['rain_repo_int'] if 'gdp_pred_from_rain' in qdf.columns else feat_cols + ['rain_repo_int']\n",
    "    y_t4, y_p4 = cv_pred(lambda: LGBMRegressor(n_estimators=300, learning_rate=0.05, max_depth=3), qdf[feat_cols3])\n",
    "    print(\"Enriched + interaction  R²:\", r2_score(y_t4, y_p4), \"MAE:\", mean_absolute_error(y_t4, y_p4))\n",
    "else:\n",
    "    print(\"Repo rate file not provided; skipping RQ-4.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
